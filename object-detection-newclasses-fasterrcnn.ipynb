{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8451492,"sourceType":"datasetVersion","datasetId":5036687},{"sourceId":8451621,"sourceType":"datasetVersion","datasetId":5036781},{"sourceId":55689,"sourceType":"modelInstanceVersion","modelInstanceId":46814}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\nimport torchvision\nimport torchvision.transforms as T\nfrom collections import defaultdict, deque\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nimport ast\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nimport torchvision.transforms as transforms\nimport cv2\nimport os,sys,matplotlib,re\nfrom PIL import Image\nfrom skimage import exposure\nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom torchvision.io import read_image\nimport matplotlib\nfrom torchvision.utils import draw_bounding_boxes\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:35:54.721426Z","iopub.execute_input":"2024-05-24T17:35:54.721774Z","iopub.status.idle":"2024-05-24T17:36:02.044702Z","shell.execute_reply.started":"2024-05-24T17:35:54.721743Z","shell.execute_reply":"2024-05-24T17:36:02.043915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.046663Z","iopub.execute_input":"2024-05-24T17:36:02.04719Z","iopub.status.idle":"2024-05-24T17:36:02.051527Z","shell.execute_reply.started":"2024-05-24T17:36:02.047156Z","shell.execute_reply":"2024-05-24T17:36:02.050538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.052494Z","iopub.execute_input":"2024-05-24T17:36:02.052742Z","iopub.status.idle":"2024-05-24T17:36:02.084051Z","shell.execute_reply.started":"2024-05-24T17:36:02.052721Z","shell.execute_reply":"2024-05-24T17:36:02.083169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_path = '/kaggle/input/tabulard/Train.csv'\nimage_folder_path = '/kaggle/input/imagest/train/train'","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.086482Z","iopub.execute_input":"2024-05-24T17:36:02.086741Z","iopub.status.idle":"2024-05-24T17:36:02.099711Z","shell.execute_reply.started":"2024-05-24T17:36:02.086719Z","shell.execute_reply":"2024-05-24T17:36:02.09892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bbox_path","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.100688Z","iopub.execute_input":"2024-05-24T17:36:02.100949Z","iopub.status.idle":"2024-05-24T17:36:02.112867Z","shell.execute_reply.started":"2024-05-24T17:36:02.100927Z","shell.execute_reply":"2024-05-24T17:36:02.112043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_bbox = pd.read_csv(bbox_path).rename(columns={ 'Xmin': 'bbox_x1', 'Xmax': 'bbox_x2', 'Ymin': 'bbox_y1', 'Ymax': 'bbox_y2'})\n\ndata_bbox['img_path']=0\n\nfor im in os.listdir(image_folder_path):\n    \n    image_id = int(im.split('.')[0].split('_')[1])\n    data_bbox.loc[data_bbox['Image_ID'] == image_id, 'img_path'] = im\ndata_bbox","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.114075Z","iopub.execute_input":"2024-05-24T17:36:02.114841Z","iopub.status.idle":"2024-05-24T17:36:02.827249Z","shell.execute_reply.started":"2024-05-24T17:36:02.114811Z","shell.execute_reply":"2024-05-24T17:36:02.82521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_bbox.ToothClass.unique()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.829426Z","iopub.execute_input":"2024-05-24T17:36:02.829946Z","iopub.status.idle":"2024-05-24T17:36:02.846025Z","shell.execute_reply.started":"2024-05-24T17:36:02.829907Z","shell.execute_reply":"2024-05-24T17:36:02.843721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_bbox.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.84851Z","iopub.execute_input":"2024-05-24T17:36:02.850292Z","iopub.status.idle":"2024-05-24T17:36:02.883218Z","shell.execute_reply.started":"2024-05-24T17:36:02.850215Z","shell.execute_reply":"2024-05-24T17:36:02.881083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_bbox['bbox_x1'][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.885815Z","iopub.execute_input":"2024-05-24T17:36:02.886395Z","iopub.status.idle":"2024-05-24T17:36:02.898412Z","shell.execute_reply.started":"2024-05-24T17:36:02.886346Z","shell.execute_reply":"2024-05-24T17:36:02.896478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i=6\nimg = read_image(image_folder_path +'/' +data_bbox['img_path'][i])\nbbox = [data_bbox['bbox_x1'][i],data_bbox['bbox_y1'][i],data_bbox['bbox_x2'][i],data_bbox['bbox_y2'][i]]\nbbox = torch.tensor(bbox, dtype=torch.int)\nbbox = bbox.unsqueeze(0)\nprint(bbox)\nimg=torchvision.utils.draw_bounding_boxes(img, bbox, width=3, colors=(255,255,0))\nimg = torchvision.transforms.ToPILImage()(img)\ndisplay(img)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:02.904831Z","iopub.execute_input":"2024-05-24T17:36:02.905401Z","iopub.status.idle":"2024-05-24T17:36:03.057895Z","shell.execute_reply.started":"2024-05-24T17:36:02.905359Z","shell.execute_reply":"2024-05-24T17:36:03.056254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = data_bbox.copy()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.060645Z","iopub.execute_input":"2024-05-24T17:36:03.061568Z","iopub.status.idle":"2024-05-24T17:36:03.070292Z","shell.execute_reply.started":"2024-05-24T17:36:03.061496Z","shell.execute_reply":"2024-05-24T17:36:03.068188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* In order to have correct implementation, classes should be from 1-numClasses (0 for background)","metadata":{}},{"cell_type":"code","source":"# df['labels']=df['ToothClass'] - df['ToothClass'].min()+1\n# Original labels\noriginal_labels = df['ToothClass'].unique().tolist()\n\n# Create the mapping dictionary\nlabel_mapper = {original_label: new_label for new_label, original_label in enumerate(sorted(original_labels), start=1)}\n\n# Apply the mapping to the DataFrame column\ndf['labels'] = df['ToothClass'].map(label_mapper)\n\nlen(df[['labels']].value_counts()), df[['labels']].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.072269Z","iopub.execute_input":"2024-05-24T17:36:03.072921Z","iopub.status.idle":"2024-05-24T17:36:03.105685Z","shell.execute_reply.started":"2024-05-24T17:36:03.072871Z","shell.execute_reply":"2024-05-24T17:36:03.104031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.labels.min()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.108322Z","iopub.execute_input":"2024-05-24T17:36:03.108867Z","iopub.status.idle":"2024-05-24T17:36:03.120111Z","shell.execute_reply.started":"2024-05-24T17:36:03.108819Z","shell.execute_reply":"2024-05-24T17:36:03.11788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.labels","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.123418Z","iopub.execute_input":"2024-05-24T17:36:03.124161Z","iopub.status.idle":"2024-05-24T17:36:03.138703Z","shell.execute_reply.started":"2024-05-24T17:36:03.124055Z","shell.execute_reply":"2024-05-24T17:36:03.136686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df['img_path'].unique().tolist())","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.141825Z","iopub.execute_input":"2024-05-24T17:36:03.142464Z","iopub.status.idle":"2024-05-24T17:36:03.157543Z","shell.execute_reply.started":"2024-05-24T17:36:03.142412Z","shell.execute_reply":"2024-05-24T17:36:03.155431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create data class:","metadata":{}},{"cell_type":"code","source":"image_id","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.160106Z","iopub.execute_input":"2024-05-24T17:36:03.161037Z","iopub.status.idle":"2024-05-24T17:36:03.172695Z","shell.execute_reply.started":"2024-05-24T17:36:03.160978Z","shell.execute_reply":"2024-05-24T17:36:03.17068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv2.imread('/kaggle/input/imagest/train/train/ID_000000.png',cv2.IMREAD_COLOR)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.175494Z","iopub.execute_input":"2024-05-24T17:36:03.177336Z","iopub.status.idle":"2024-05-24T17:36:03.207829Z","shell.execute_reply.started":"2024-05-24T17:36:03.177248Z","shell.execute_reply":"2024-05-24T17:36:03.206065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['img_path'].unique().tolist()[9]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.210043Z","iopub.execute_input":"2024-05-24T17:36:03.210756Z","iopub.status.idle":"2024-05-24T17:36:03.224099Z","shell.execute_reply.started":"2024-05-24T17:36:03.210695Z","shell.execute_reply":"2024-05-24T17:36:03.222191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CarDataset(object):\n    def __init__(self, df, IMG_DIR, transforms=None):\n        self.a = 0\n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df['img_path'].unique().tolist()\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.image_ids)\n        \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n#         print(self.img_dir+image_id)\n        records = self.df[self.df['img_path'] == image_id]\n        \n        image = cv2.imread(self.img_dir+'//'+image_id,cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n\n#         print(image)\n        image /= 255.0\n#         display(records)\n        boxes = records[['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].to_numpy()\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        labels =  records['labels'].to_numpy()#torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = torch.as_tensor(area, dtype=torch.float32)\n        target['iscrowd'] = torch.zeros((records.shape[0],), dtype=torch.int64)\n    \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n        return image.clone().detach(), target, image_id","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:36:03.227214Z","iopub.execute_input":"2024-05-24T17:36:03.227805Z","iopub.status.idle":"2024-05-24T17:36:03.246034Z","shell.execute_reply.started":"2024-05-24T17:36:03.227742Z","shell.execute_reply":"2024-05-24T17:36:03.244209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n# def get_train_transform():\n#     return A.Compose([\n#         A.Flip(0.5),\n#         ToTensorV2()\n#     ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_train_transform():\n    return A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=90, p=1.0),\n        A.ColorJitter( contrast=0.8, saturation=0.8),\n        ToTensorV2()\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2()\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:18.114204Z","iopub.execute_input":"2024-05-24T18:40:18.114575Z","iopub.status.idle":"2024-05-24T18:40:18.12164Z","shell.execute_reply.started":"2024-05-24T18:40:18.114547Z","shell.execute_reply":"2024-05-24T18:40:18.120673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let`s show how work our class dataset","metadata":{}},{"cell_type":"code","source":"CD = CarDataset(df, image_folder_path, get_train_transform())\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:19.188819Z","iopub.execute_input":"2024-05-24T18:40:19.189172Z","iopub.status.idle":"2024-05-24T18:40:19.195035Z","shell.execute_reply.started":"2024-05-24T18:40:19.189144Z","shell.execute_reply":"2024-05-24T18:40:19.194088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to visualize a single image and its bounding boxes\ndef visualize(image, target):\n    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n    ax.imshow(image)\n    \n    boxes = target['boxes'].cpu().numpy()\n    for box in boxes:\n        x1, y1, x2, y2 = box\n        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, color='red', linewidth=2)\n        ax.add_patch(rect)\n    plt.show()\n\n# Visualize a few samples\nfor i in range(5):\n    image, target, image_id = CD[i]\n    print(f\"Image ID: {image_id}\")\n    visualize(image, target)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:19.449245Z","iopub.execute_input":"2024-05-24T18:40:19.449517Z","iopub.status.idle":"2024-05-24T18:40:21.3222Z","shell.execute_reply.started":"2024-05-24T18:40:19.449493Z","shell.execute_reply":"2024-05-24T18:40:21.321284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CD[10]","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:21.905621Z","iopub.execute_input":"2024-05-24T18:40:21.906441Z","iopub.status.idle":"2024-05-24T18:40:21.965849Z","shell.execute_reply.started":"2024-05-24T18:40:21.906402Z","shell.execute_reply":"2024-05-24T18:40:21.964859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(14, 10))\nsample = CD[1]\n\n# print(sample[1])\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:26.708172Z","iopub.execute_input":"2024-05-24T18:40:26.708971Z","iopub.status.idle":"2024-05-24T18:40:27.200348Z","shell.execute_reply.started":"2024-05-24T18:40:26.70894Z","shell.execute_reply":"2024-05-24T18:40:27.199466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    # Extract relevant data\n    sample = CD[i]\n\n    image_tensor = sample[0]\n    boxes = sample[1]['boxes']\n    labels = sample[1]['labels']\n\n    # Convert image tensor to uint8\n    img_uint8 = image_tensor.permute(1, 2, 0).numpy() * 255\n    img_uint8 = img_uint8.astype(np.uint8)\n\n    # Plot the image\n    plt.figure(figsize=(14, 10))\n    plt.imshow(img_uint8)\n\n    # Draw bounding boxes with labels\n    for box, label in zip(boxes, labels):\n        x_min, y_min, x_max, y_max = box\n        plt.gca().add_patch(plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n                                          linewidth=2, edgecolor='r', facecolor='none'))\n        plt.text(x_min, y_min - 5, f'{label}', color='r', fontsize=12, fontweight='bold')\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:31.004988Z","iopub.execute_input":"2024-05-24T18:40:31.005866Z","iopub.status.idle":"2024-05-24T18:40:32.989525Z","shell.execute_reply.started":"2024-05-24T18:40:31.005834Z","shell.execute_reply":"2024-05-24T18:40:32.988513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split data and create dataloaders:","metadata":{}},{"cell_type":"code","source":"image_ids = df['img_path'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\nvalid_df = df[df['img_path'].isin(valid_ids)]\ntrain_df = df[df['img_path'].isin(train_ids)]\ntrain_df.shape,valid_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:32.991418Z","iopub.execute_input":"2024-05-24T18:40:32.991749Z","iopub.status.idle":"2024-05-24T18:40:33.006822Z","shell.execute_reply.started":"2024-05-24T18:40:32.991721Z","shell.execute_reply":"2024-05-24T18:40:33.005862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = CarDataset(train_df, image_folder_path, get_train_transform())\nvalid_dataset = CarDataset(valid_df, image_folder_path, get_valid_transform())\n\nindices = torch.randperm(len(train_dataset)).tolist()\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=2,\n    collate_fn=collate_fn\n)\n\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=2,\n    shuffle=True,\n    num_workers=2,\n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:33.007971Z","iopub.execute_input":"2024-05-24T18:40:33.008338Z","iopub.status.idle":"2024-05-24T18:40:33.018398Z","shell.execute_reply.started":"2024-05-24T18:40:33.008306Z","shell.execute_reply":"2024-05-24T18:40:33.017592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model:\nI use fasterrcnn_resnet50_fpn pre-trained on COCO dataset","metadata":{}},{"cell_type":"code","source":"num_classes = 32+1  # 1 class (car) + background\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, backbone_name='resnet101')\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:33.170806Z","iopub.execute_input":"2024-05-24T18:40:33.171047Z","iopub.status.idle":"2024-05-24T18:40:33.986316Z","shell.execute_reply.started":"2024-05-24T18:40:33.171027Z","shell.execute_reply":"2024-05-24T18:40:33.985487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Faster-RFormer","metadata":{}},{"cell_type":"code","source":"import torchvision\n# from torchvision.models.detection import FasterRCNN\n# from torchvision.models.detection.rpn import AnchorGenerator\n# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n# # Define the backbone\n# backbone = resnet_fpn_backbone('resnet101', pretrained=True)\n\n# # Define the number of feature maps generated by the backbone\n# num_anchors = 5  # Number of anchor scales (corresponding to 5 feature maps)\n\n# # Define the anchor generator\n# anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n# aspect_ratios = ((0.5, 1.0, 2.0),) * num_anchors\n# anchor_generator = AnchorGenerator(sizes=anchor_sizes,\n#                                    aspect_ratios=aspect_ratios)\n\n# # Define the ROI pooler\n# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n#                                                 output_size=7,\n#                                                 sampling_ratio=2)\n\n# # Create the Faster R-CNN model\n# model = FasterRCNN(backbone=backbone,\n#                    num_classes=num_classes,\n#                    rpn_anchor_generator=anchor_generator,\n#                    box_roi_pool=roi_pooler)\n\n# # Modify the box predictor to match the number of classes\n# in_features = model.roi_heads.box_predictor.cls_score.in_features\n# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:40:34.785729Z","iopub.execute_input":"2024-05-24T18:40:34.786446Z","iopub.status.idle":"2024-05-24T18:40:34.791658Z","shell.execute_reply.started":"2024-05-24T18:40:34.786413Z","shell.execute_reply":"2024-05-24T18:40:34.790626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:53:51.873207Z","iopub.execute_input":"2024-05-24T18:53:51.873554Z","iopub.status.idle":"2024-05-24T18:53:51.884653Z","shell.execute_reply.started":"2024-05-24T18:53:51.873526Z","shell.execute_reply":"2024-05-24T18:53:51.883806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:53:52.148233Z","iopub.execute_input":"2024-05-24T18:53:52.148606Z","iopub.status.idle":"2024-05-24T18:53:52.152768Z","shell.execute_reply.started":"2024-05-24T18:53:52.148578Z","shell.execute_reply":"2024-05-24T18:53:52.151855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train our model:","metadata":{}},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport sys\nfrom tqdm import tqdm\nmodel.to(device)\n\n# # Path to the saved model weights\n# saved_weights_path = '/kaggle/input/weights27/pytorch/modelweights/1/epoch_27_best_weights.pth'\n\n# # Load the model weights\n# model.load_state_dict(torch.load(saved_weights_path))\n\ndef train_one_epoch(model, optimizer, loader, device, epoch):\n    model.to(device)\n    model.train()\n    \n    all_losses = []\n    all_losses_dict = []\n    \n    for images, targets, _ in tqdm(loader):\n        images = list(image.to(device) for image in images)\n        targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n        \n        loss_dict = model(images, targets)\n#         if zinka==0:\n#             zinka+=1\n#             print(loss_dict)\n#             print(type(loss_dict))\n        losses = sum(loss for loss in loss_dict.values())\n        loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n        loss_value = losses.item()\n        \n        all_losses.append(loss_value)\n        all_losses_dict.append(loss_dict_append)\n        \n        if not math.isfinite(loss_value):\n            print(f\"Loss is {loss_value}, stopping training\")\n            print(loss_dict)\n            sys.exit(1)\n        \n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n    all_losses_dict = pd.DataFrame(all_losses_dict)\n    avg_loss = np.mean(all_losses)\n    print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        epoch, optimizer.param_groups[0]['lr'], avg_loss,\n        all_losses_dict['loss_classifier'].mean(),\n        all_losses_dict['loss_box_reg'].mean(),\n        all_losses_dict['loss_rpn_box_reg'].mean(),\n        all_losses_dict['loss_objectness'].mean()\n    ))\n    return avg_loss\n\nglobal_loss_dict = None\ndef validate_one_epoch(model, loader, device):\n    global global_loss_dict\n    model.to(device)\n#     model.eval()# never use val here because it make model(images, targets) make prediction instead on computing loss\n    \n    all_losses = []\n    all_losses_dict = []\n    \n    with torch.no_grad():\n        for images, targets, _ in tqdm(loader):\n            images = list(image.to(device) for image in images)\n            targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n            loss_dict = model(images, targets)\n            global_loss_dict = loss_dict\n#             print('loss_dict',loss_dict)\n            \n            if isinstance(loss_dict, dict):\n                losses = sum(loss for loss in loss_dict.values())\n                loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n                loss_value = losses.item()\n            else:\n                print(\"Unexpected loss_dict format: \", type(loss_dict))\n                loss_value = loss_dict.item()\n                loss_dict_append = {\"loss\": loss_value}\n            \n            all_losses.append(loss_value)\n            all_losses_dict.append(loss_dict_append)\n    \n    all_losses_dict = pd.DataFrame(all_losses_dict)\n    avg_loss = np.mean(all_losses)\n    print(\"Validation loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        avg_loss,\n        all_losses_dict.get('loss_classifier', pd.Series([0])).mean(),\n        all_losses_dict.get('loss_box_reg', pd.Series([0])).mean(),\n        all_losses_dict.get('loss_rpn_box_reg', pd.Series([0])).mean(),\n        all_losses_dict.get('loss_objectness', pd.Series([0])).mean()\n    ))\n    return all_losses_dict.get('loss_box_reg', pd.Series([0])).mean()\n\ndef train_model(model, optimizer, train_loader, val_loader, num_epochs, device, patience):\n    train_losses = []\n    val_losses = []\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n\n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n        train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n        val_loss = validate_one_epoch(model, val_loader, device)\n        \n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            # Save model weights\n            torch.save(model.state_dict(), f'epoch_{epoch}_best_weights.pth')\n        else:\n            patience_counter += 1\n\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch + 1}\")\n            break\n    \n    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss over Epochs')\n    plt.legend()\n    plt.show()\n\n# Example usage:\nnum_epochs = 50\npatience = 20\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# Assume train_loader and val_loader are your training and validation data loaders\n# Example: train_loader = ...\n#          val_loader = ...\ntrain_model(model, optimizer, train_data_loader, valid_data_loader, num_epochs, device, patience)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T18:53:52.932332Z","iopub.execute_input":"2024-05-24T18:53:52.932705Z","iopub.status.idle":"2024-05-24T20:13:16.004055Z","shell.execute_reply.started":"2024-05-24T18:53:52.932667Z","shell.execute_reply":"2024-05-24T20:13:16.003026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load weights","metadata":{}},{"cell_type":"code","source":"\n# model.to(device)\n\n# # Path to the saved model weights\n# saved_weights_path = '/kaggle/input/weights27/pytorch/modelweights/1/epoch_27_best_weights.pth'\n\n# # Load the model weights\n# model.load_state_dict(torch.load(saved_weights_path))\n\n# # Set the model to evaluation mode if you're using it for inference\n# model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T17:45:35.3597Z","iopub.status.idle":"2024-05-24T17:45:35.360012Z","shell.execute_reply.started":"2024-05-24T17:45:35.35986Z","shell.execute_reply":"2024-05-24T17:45:35.359873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show resault:","metadata":{}},{"cell_type":"code","source":"def apply_nms(orig_prediction, iou_thresh=0.2):\n    \n\n    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n    \n    final_prediction = orig_prediction\n    final_prediction['boxes'] = final_prediction['boxes'][keep]\n    final_prediction['scores'] = final_prediction['scores'][keep]\n    final_prediction['labels'] = final_prediction['labels'][keep]\n    \n    return final_prediction","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:47:48.137609Z","iopub.execute_input":"2024-05-24T20:47:48.138032Z","iopub.status.idle":"2024-05-24T20:47:48.144483Z","shell.execute_reply.started":"2024-05-24T20:47:48.137994Z","shell.execute_reply":"2024-05-24T20:47:48.143584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img,target,_ = valid_dataset[1]\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to('cuda')])[0]\n    \nnms_prediction = apply_nms(prediction, iou_thresh=0.7)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:47:48.552089Z","iopub.execute_input":"2024-05-24T20:47:48.552643Z","iopub.status.idle":"2024-05-24T20:47:48.633699Z","shell.execute_reply.started":"2024-05-24T20:47:48.552615Z","shell.execute_reply":"2024-05-24T20:47:48.632807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nms_prediction","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:47:50.101836Z","iopub.execute_input":"2024-05-24T20:47:50.102499Z","iopub.status.idle":"2024-05-24T20:47:50.113184Z","shell.execute_reply.started":"2024-05-24T20:47:50.102469Z","shell.execute_reply":"2024-05-24T20:47:50.112331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"REAL BOXES\")\nfig = plt.figure(figsize=(14, 10))\nsample = valid_dataset[10]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:47:51.694164Z","iopub.execute_input":"2024-05-24T20:47:51.694507Z","iopub.status.idle":"2024-05-24T20:47:52.176819Z","shell.execute_reply.started":"2024-05-24T20:47:51.694481Z","shell.execute_reply":"2024-05-24T20:47:52.175915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"PREDICTED BOXES\")\nfig = plt.figure(figsize=(14, 10))\nsample = valid_dataset[109]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(img_int,\n    nms_prediction['boxes'][nms_prediction['scores'] > 0.5], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:01.956091Z","iopub.execute_input":"2024-05-24T20:48:01.956763Z","iopub.status.idle":"2024-05-24T20:48:02.384778Z","shell.execute_reply.started":"2024-05-24T20:48:01.956717Z","shell.execute_reply":"2024-05-24T20:48:02.383806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv('/kaggle/input/tabulard/Test.csv').copy()\ntest_df['img_path']=test_df.Image_ID\ntest_df.drop(columns=['Image_ID'],inplace=True)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:08.865329Z","iopub.execute_input":"2024-05-24T20:48:08.865923Z","iopub.status.idle":"2024-05-24T20:48:08.888234Z","shell.execute_reply.started":"2024-05-24T20:48:08.865894Z","shell.execute_reply":"2024-05-24T20:48:08.887164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CarTestDataset(object):\n#     def __init__(self, df, IMG_DIR, transforms=None):\n#         self.df = df\n#         self.img_dir = IMG_DIR\n#         self.image_ids = self.df['img_path'].unique().tolist()\n#         self.transforms = transforms\n#         print('hello')\n#     def __len__(self):\n#         return len(self.image_ids)\n        \n#     def __getitem__(self, idx):\n#         image_id = self.image_ids[idx]\n#         print(image_id)\n#         records = self.df[self.df['img_path'] == image_id]\n#         print(os.path.join(self.img_dir, image_id, '.png'))\n#         image = cv2.imread(os.path.join(self.img_dir, image_id)+'.png', cv2.IMREAD_COLOR)\n#         print('im',image)\n#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n#         image /= 255.0\n#         print(image)\n#         # For test set, you don't have ground truth annotations, so return dummy targets\n#         target = {}\n\n#         if self.transforms:\n#             sample = {\n#                 'image': image,\n#             }\n#             sample = self.transforms(**sample)\n#             image = sample['image']\n            \n#         return image.clone().detach(), image_id\n# IMG_DIR_TEST = '/kaggle/input/imagest/test/test'\n# def get_valid_transform():\n#     return A.Compose([\n#         ToTensorV2()\n#     ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\n# # Example of how to use the test dataset with transformations\n# test_transform = get_valid_transform()  # You can use valid transform or define a new transform\n# test_dataset = CarTestDataset(test_df, IMG_DIR_TEST, transforms=test_transform)\n# test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:09.476021Z","iopub.execute_input":"2024-05-24T20:48:09.47635Z","iopub.status.idle":"2024-05-24T20:48:09.481604Z","shell.execute_reply.started":"2024-05-24T20:48:09.476324Z","shell.execute_reply":"2024-05-24T20:48:09.480732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nclass CarTestDataset(object):\n    def __init__(self, df, IMG_DIR, transforms=None):\n        self.df = df\n        self.img_dir = IMG_DIR\n        self.image_ids = self.df['img_path'].unique().tolist()\n        self.transforms = transforms\n        print('hello')\n        \n    def __len__(self):\n        return len(self.image_ids)\n        \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n#         print(\"image_id:\", image_id)\n#         print(self.img_dir, image_id)\n        \n        image_path = os.path.join(self.img_dir, image_id)+ '.png'\n#         print(\"image_path:\", image_path)\n        \n        if not os.path.exists(image_path):\n            raise FileNotFoundError(f\"Image file not found: {image_path}\")\n        \n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n#         print(\"image:\", image)\n        \n        if image is None:\n            raise IOError(f\"Unable to read image file: {image_path}\")\n        \n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n        \n#         print(\"transformed image:\", image)\n        \n        # For test set, you don't have ground truth annotations, so return dummy targets\n        target = {}\n\n        if self.transforms:\n            sample = {\n                'image': image,\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n        return image.clone().detach(), image_id\n\n# Example of how to use the test dataset with transformations\nIMG_DIR_TEST = '/kaggle/input/imagest/test/test'\n\ntest_dataset = CarTestDataset(test_df, IMG_DIR_TEST)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:10.103968Z","iopub.execute_input":"2024-05-24T20:48:10.104664Z","iopub.status.idle":"2024-05-24T20:48:10.116656Z","shell.execute_reply.started":"2024-05-24T20:48:10.104634Z","shell.execute_reply":"2024-05-24T20:48:10.115783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example of how to use the test dataset with transformations\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2()\n    ])\n\ntest_transform = get_valid_transform()  # You can use valid transform or define a new transform\ntest_dataset = CarTestDataset(test_df, IMG_DIR_TEST, transforms=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n\n# Visual inspection of a few samples\nfor i in range(5):\n    image, image_id = test_dataset[i]\n    print(\"Sample\", i)\n    print(\"Image ID:\", image_id)\n    print(\"Image shape:\", image.shape)\n    plt.imshow(image.permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C) for visualization\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:16.85082Z","iopub.execute_input":"2024-05-24T20:48:16.851208Z","iopub.status.idle":"2024-05-24T20:48:18.257725Z","shell.execute_reply.started":"2024-05-24T20:48:16.851176Z","shell.execute_reply":"2024-05-24T20:48:18.256794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img,target = test_dataset[77]\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to('cuda')])[0]\n    \nnms_prediction = apply_nms(prediction, iou_thresh=0.7)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:20.710776Z","iopub.execute_input":"2024-05-24T20:48:20.711658Z","iopub.status.idle":"2024-05-24T20:48:20.791436Z","shell.execute_reply.started":"2024-05-24T20:48:20.711625Z","shell.execute_reply":"2024-05-24T20:48:20.790454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"PREDICTED BOXES\")\nfig = plt.figure(figsize=(14, 10))\nsample = test_dataset[77]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(img_int,\n    nms_prediction['boxes'][nms_prediction['scores'] > 0.25], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:21.110477Z","iopub.execute_input":"2024-05-24T20:48:21.1108Z","iopub.status.idle":"2024-05-24T20:48:21.572888Z","shell.execute_reply.started":"2024-05-24T20:48:21.110773Z","shell.execute_reply":"2024-05-24T20:48:21.571875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"PREDICTED BOXES\")\nfig = plt.figure(figsize=(14, 10))\nsample = test_dataset[77]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\nplt.imshow(draw_bounding_boxes(img_int,\n    nms_prediction['boxes'][nms_prediction['scores'] > 0.5], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:35.730569Z","iopub.execute_input":"2024-05-24T20:48:35.731478Z","iopub.status.idle":"2024-05-24T20:48:36.19909Z","shell.execute_reply.started":"2024-05-24T20:48:35.731446Z","shell.execute_reply":"2024-05-24T20:48:36.198243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting All images in test","metadata":{}},{"cell_type":"code","source":"nms_prediction['labels']","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:43.329301Z","iopub.execute_input":"2024-05-24T20:48:43.330062Z","iopub.status.idle":"2024-05-24T20:48:43.337054Z","shell.execute_reply.started":"2024-05-24T20:48:43.330031Z","shell.execute_reply":"2024-05-24T20:48:43.336165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reverse_label_mapper = {v: k for k, v in label_mapper.items()}\noriginal_labels = [reverse_label_mapper[label.item()] for label in nms_prediction['labels']]\n\nprint(original_labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:43.724426Z","iopub.execute_input":"2024-05-24T20:48:43.724729Z","iopub.status.idle":"2024-05-24T20:48:43.731002Z","shell.execute_reply.started":"2024-05-24T20:48:43.724706Z","shell.execute_reply":"2024-05-24T20:48:43.730063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Create an empty list to store the results\nresults = []\n\n# Iterate over all images in the test dataset\nfor idx in range(len(test_dataset)):\n    img, img_id = test_dataset[idx]\n    \n    # Perform inference\n    model.eval()\n    with torch.no_grad():\n        prediction = model([img.to('cuda')])[0]\n\n    # Apply non-maximum suppression\n    nms_prediction = apply_nms(prediction, iou_thresh=0.7)\n    \n    # Extract bounding box coordinates and scores\n    boxes = nms_prediction['boxes'][nms_prediction['scores'] > 0.5]\n    scores = nms_prediction['scores'][nms_prediction['scores'] > 0.5]\n    labels = nms_prediction['labels'][nms_prediction['scores'] > 0.5]\n    labels = [reverse_label_mapper[label.item()] for label in labels]\n    # Convert image ID and bounding box coordinates to DataFrame\n    for i in range(len(boxes)):\n        result = {\n            'image_id': img_id,\n            'xmin': boxes[i][0].item(),\n            'ymin': boxes[i][1].item(),\n            'xmax': boxes[i][2].item(),\n            'ymax': boxes[i][3].item(),\n            'score': scores[i].item(),\n            'class':labels[i]\n        }\n        results.append(result)\n\n# Convert the list of dictionaries to a DataFrame\nresults_df = pd.DataFrame(results)\n\n# Display the DataFrame\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:48:53.828921Z","iopub.execute_input":"2024-05-24T20:48:53.829292Z","iopub.status.idle":"2024-05-24T20:49:35.965478Z","shell.execute_reply.started":"2024-05-24T20:48:53.829264Z","shell.execute_reply":"2024-05-24T20:49:35.964627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df['Image_ID']=results_df['image_id']\nresults_df.drop(columns=['image_id'],inplace=True)\nresults_df['class']= results_df['class'].apply(lambda x: 'class_'+str(x))\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:49:35.966808Z","iopub.execute_input":"2024-05-24T20:49:35.967058Z","iopub.status.idle":"2024-05-24T20:49:35.987509Z","shell.execute_reply.started":"2024-05-24T20:49:35.967037Z","shell.execute_reply":"2024-05-24T20:49:35.986675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df['confidence']=results_df['score']\nresults_df=results_df.drop(columns=['score'])\nresults_df","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:49:35.98859Z","iopub.execute_input":"2024-05-24T20:49:35.988883Z","iopub.status.idle":"2024-05-24T20:49:36.011562Z","shell.execute_reply.started":"2024-05-24T20:49:35.988858Z","shell.execute_reply":"2024-05-24T20:49:36.010735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df.Image_ID.nunique()","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:49:36.013567Z","iopub.execute_input":"2024-05-24T20:49:36.01382Z","iopub.status.idle":"2024-05-24T20:49:36.02031Z","shell.execute_reply.started":"2024-05-24T20:49:36.013798Z","shell.execute_reply":"2024-05-24T20:49:36.019424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission_file= pd.read_csv('/kaggle/input/tabulard/SampleSubmission.csv')\nsubmission_file","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:49:36.021113Z","iopub.execute_input":"2024-05-24T20:49:36.021384Z","iopub.status.idle":"2024-05-24T20:49:36.045767Z","shell.execute_reply.started":"2024-05-24T20:49:36.021354Z","shell.execute_reply":"2024-05-24T20:49:36.044916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df.to_csv('teeth_detection_00_.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-24T20:49:36.046907Z","iopub.execute_input":"2024-05-24T20:49:36.047481Z","iopub.status.idle":"2024-05-24T20:49:36.128183Z","shell.execute_reply.started":"2024-05-24T20:49:36.04745Z","shell.execute_reply":"2024-05-24T20:49:36.127436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Upvote this this notebook was helpful! Thanks","metadata":{}}]}